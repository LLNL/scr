#!/bin/bash

# Run this script after the final run in a job allocation
# to scavenge files from cache to parallel file system.

# if SCR is disabled, immediately exit
if [ "$SCR_ENABLE" == "0" ] ; then
  exit 0
fi

# if SCR_DEBUG is set > 0, turn on verbosity
verbose=""
if [ -n "$SCR_DEBUG" ]; then
  if [ $SCR_DEBUG -gt 0 ] ; then
    set -x
    verbose="--verbose"
  fi
fi

# record the start time for timing purposes
start_time=`date`
start_secs=`date +%s`

bindir="@X_BINDIR@"

prog="scr_postrun"

print_usage() { echo "Usage: $prog [-p prefix_dir]"; exit 1; }

# pass prefix via command line
pardir=${SCR_PREFIX:-`pwd`}
OPTIND=1
while getopts "p:" flag ; do
  case $flag in
    p) pardir=$OPTARG;;  # parallel file system prefix
    *) print_usage;;
  esac
done

# check that we have the parallel file system prefix
if [ "$pardir" == "" ] ; then
  print_usage
fi

# all parameters checked out, start normal output
echo "$prog: Started: $start_time"

# get our nodeset for this job
if [ -z "$SCR_NODELIST" ] ; then
  nodelist_env=`$bindir/scr_env --nodes`
  if [ $? -eq 0 ] ; then
    SCR_NODELIST=$nodelist_env
  fi
fi
if [ -z "$SCR_NODELIST" ] ; then
  echo "$prog: ERROR: Could not identify nodeset"
  exit 1
fi
export SCR_NODELIST

# identify what nodes are still up
UPNODES=$SCR_NODELIST
DOWNNODES=`$bindir/scr_list_down_nodes $SCR_NODELIST`
if [ "$DOWNNODES" ]; then
  UPNODES=`$bindir/scr_glob_hosts -m $SCR_NODELIST:$DOWNNODES`
fi
echo "$prog: UPNODES:   $UPNODES"

# if there is at least one remaining up node, attempt to scavenge
ret=1
if [ "$UPNODES" != "" ] ; then
  # get the SCR control directory
  cntldir=`$bindir/scr_cntl_dir`;
  # TODO: check that we have a control directory

  # TODODEST: need to scavenge latest checkpoint and any output set

  # check whether we have a dataset set to flush
  echo "$prog: Looking for latest dataset set id"
  latestid=`$bindir/scr_flush_file --dir $pardir --latest`
  if [ $? -eq 0 ] ; then
    # we have a dataset, check whether it still needs to be flushed 
    $bindir/scr_flush_file --dir $pardir --needflush $latestid
    if [ $? -eq 0 ] ; then
#      # get the jobid
#      jobid=`$bindir/scr_env --jobid`
#      # TODO: check that we got the jobid

      # get list of possible datasets
      dataset_list=`$bindir/scr_inspect --up $UPNODES --from $cntldir`
      if [ $? -eq 0 ] ; then
        for d in $dataset_list ; do
          # determine whether this dataset needs to be flushed
          $bindir/scr_flush_file --dir $pardir --needflush $d
          if [ $? -eq 0 ] ; then
            echo "$prog: Attempting to scavenge dataset $d"

            # make a new directory to store the files for this dataset
#            timestamp=`date +%F_%H:%M:%S`
#            scrdir="scr.${timestamp}.${jobid}.${d}"
            scrdir="scr.dataset.${d}"
            datadir=$pardir/$scrdir
            mkdir -p $datadir

            # Gather files from cache to parallel file system
            echo "$prog: Scavenging files from cache to $datadir"
            echo $prog: $bindir/scr_scavenge $verbose --id $d --from $cntldir --to $datadir --jobset $SCR_NODELIST --up $UPNODES
            $bindir/scr_scavenge $verbose --id $d --from $cntldir --to $datadir --jobset $SCR_NODELIST --up $UPNODES
            echo "$prog: Done scavenging files from cache to $datadir"

            # check that gathered set is complete, if not, don't update current symlink
            update_current=1
            echo "$prog: Checking that dataset is complete"
            echo "$bindir/scr_index --prefix $pardir --add $scrdir"
            $bindir/scr_index --prefix $pardir --add $scrdir
            if [ $? -ne 0 ] ; then
              update_current=0 # incomplete dataset, don't update current symlink
            fi

            # if the set is complete, update the current symlink
            if [ "$update_current" == "1" ] ; then
              echo "$prog: Updating scr.current symlink"
              current="$pardir/scr.current"

              # if there is a current, remove it
              if [ -e $current ] ; then
                rm -f $current
              fi

              # make the new current
              echo "$prog:   $current --> $scrdir"
              ln -s $scrdir $current

              # just completed scavenging this dataset, so quit
              ret=0
              break;
            fi
          else
            # found a dataset that has already been flushed, we can quit
            echo "$prog: Dataset $d has already been flushed"
            ret=0
            break;
          fi
        done
      else
        echo "$prog: Failed to inspect cache or cannot scavenge any datasets"
      fi
    else
      echo "$prog: Latest dataset set has already been flushed"
      ret=0
    fi
  else
    echo "$prog: Found no dataset set to scavenge"
  fi
fi

# print the timing info
end_time=`date`
end_secs=`date +%s`
run_secs=$(($end_secs - $start_secs))
echo "$prog: Ended: $end_time"
echo "$prog: secs: $run_secs"

# print the exit code and exit
echo "$prog: exit code: $ret"
exit $ret
